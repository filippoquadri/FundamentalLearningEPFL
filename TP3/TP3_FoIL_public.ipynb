{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiE4Goe58ycm"
      },
      "source": [
        "# EE-411 Fundamentals of inference and learning, EPFL \n",
        "## Exercise Session 3: Classification using k-Nearest Neighbors\n",
        "\n",
        "In this third set of exercises we will start to see an application of supervised learning, introducing one of the simplest-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems: **k-Nearest Neighbors (KNN)**.\n",
        "\n",
        "**What you will learn today:** In this third session, we will construct the kNN algorithm by hand, explaining different ways in which you can compute the distance between elements of a matrix. We will compute the train error and the test error, and we will see how they behave as a function of the parameter $k$ or of the degrees of freedom $N/k$. Then, we will introduce **scikit-learn** that allows us to implement kNN (and much more) in a very simple way. Finally, we will apply what we have learnt to a real, and very famous, dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GRwyNOsUQwjT"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "np.random.seed(1234)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = (9, 9)\n",
        "plt.rcParams[\"font.size\"] = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBHvZXS_8yco"
      },
      "source": [
        "Let us first generate some synthetic data. We will assume that we **know** what is the probability distribution of our datapoints, pay attention that usually we do not know it! \n",
        "Our **Generative Model** will be a Gaussian mixture model (GMM): \n",
        "* Assume that centroids from class zero and one are geneterated Gaussianly as follows\n",
        " $${\\vec m}_k^{(0)} \\sim \\mathcal{N} (\\vec{\\mu} = [1,0] ; I_2) \\qquad {\\vec m}_k^{(1)} \\sim \\mathcal{N} (\\vec{\\mu} = [0,1] ; I_2) $$ \n",
        "* For each class, sample 10 different centroids ${\\vec m}_k$\n",
        "* Sample the actual data from $\\mathcal{N} ({\\vec m}_k, \\frac{1}{5} I_2)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePQjGeEWrPXK"
      },
      "source": [
        "Let's sample the centroids and plot them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coding together #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fxlujwL48ycq"
      },
      "source": [
        "Once we have the centroids we can sample the actual data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coding together #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxykyPKD8ycr"
      },
      "source": [
        "This is the problem that we want to solve: we are given these points and we want to find a decision boundary that ensures good generalization.\n",
        "This is in general our goal: we want to be able to correctly classify new samples, never seen before, once they are given to us.\n",
        "\n",
        "Let us group the data in a nice way. For binary classification problems like this one, the way data is usually arranged is in\n",
        "\n",
        "- a matrix $X$ of size $N \\times P$, where $N$ is the number of samples and $P$ is the number of features (in our case $N = 200$ and $P = 2$); the nomenclature \"feature\" is not always clear but usually we mean with it the dimension of the space in which the datapoints leave.\n",
        "- a label vector $y \\in \\{0, 1\\}^N$ saying to which class each sample belongs to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsiauBY18ycs",
        "outputId": "34c5c6fe-4ea5-4b3e-f74c-844c0e7783eb"
      },
      "outputs": [],
      "source": [
        "# Coding together #\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FD8ctEK8yct"
      },
      "source": [
        "Next we compute the distance matrix, a $N \\times N$ matrix containing the distance from each sample to all others (do we really need to?)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kg8_DLmA8ycu"
      },
      "outputs": [],
      "source": [
        "# Coding together #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw-KKAS7ac8O"
      },
      "source": [
        "If you know already Python, you can recognize that it is not an efficient implementation. \n",
        "\n",
        "We wanto to minimize *for* loops in Python! This is a general rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVwhZ6bwayxi",
        "outputId": "587fe05e-8967-4774-d4ef-be0a2fa02d9d"
      },
      "outputs": [],
      "source": [
        "# Coding together #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJYAe6Z78ycv",
        "outputId": "237ad1db-8585-496e-9d79-eb24e36bce2a"
      },
      "outputs": [],
      "source": [
        "# Coding together #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY-hNxvK8ycv"
      },
      "source": [
        "Using the distance matrix we can now write our algorithm!\n",
        "\n",
        "**Workflow**: \n",
        "* Write a function that compute the $k$-nearest neighbor estimate for each point in the training set.\n",
        "* Use this function to assign a class to each point by majority vote choosing a $k$ of your choice\n",
        "* Compute the *training error* \n",
        "\n",
        "\n",
        "**Tip**: look up for the `np.argpartition` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "F0nGxxrG8ycw"
      },
      "outputs": [],
      "source": [
        "# Coding together #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4dtEWXwzELn"
      },
      "source": [
        "Let's write down the function which compute the $k$-nearest neighbour for each datapoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coding together #\n",
        "# def knn(X, y, k):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P85fXoJqzaQG"
      },
      "source": [
        "Now compute the estimated labels for a given instance $(X,y)$ at a fixed $k$ (e.g. $k=10$).\n",
        "\n",
        "Once we have the estimated labels we are ready to compute the *training error*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5nYHaUNy_b_",
        "outputId": "41bc30d3-3d02-4704-bf57-f3772cd146b3"
      },
      "outputs": [],
      "source": [
        "# Coding together #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL00eKlz8ycx"
      },
      "source": [
        "The first half of the vector should be composed solely of 0's, and the second half should be composed of 1's; we can see however that there are some mistakes. Let us plot them to try to understand what is happening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "G7k93_7N8ycx",
        "outputId": "d5b0cf4b-ae1c-48db-c7f2-40b9a0c7ac6d"
      },
      "outputs": [],
      "source": [
        "# Coding together #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtlu_3Zj8ycx"
      },
      "source": [
        "As expected, mistakes happen in regions where the majority of points belong to the other class.\n",
        "\n",
        "But what happens when we try to classify points outside the training set? \n",
        "\n",
        "This step is called the **test step**. This is crucial and we must be very careful while doing this. There are a few different ways of assessing this. For instance, we could have used only part of our data in the training set (usually around 80%), and use the remaining to compute the so-called **test error**.Since in our case however the generative model is known, we might as well just get more samples from it.\n",
        "\n",
        "\n",
        "$\\color{darkred}{Caveat}$: Never use samples used in the training step to assess the test performance!\n",
        "\n",
        "Please pay attention to the previous lines because during the following lectures we will deal always with $\\color{blue}{training}$ and $\\color{green}{test}$ error. The difference between them is the same that we encounter when considering $\\color{green}{learning}$ and  $\\color{blue}{memorizing}$: good learning means low test error **NOT** low training error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eYXAJFqT8ycx"
      },
      "outputs": [],
      "source": [
        "# Coding together #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "NpVKKLeY8ycy",
        "outputId": "4e581a7a-4818-4d37-bb39-e4b9437456aa"
      },
      "outputs": [],
      "source": [
        "# Coding together #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VirBAXrm8ycy"
      },
      "source": [
        "We now need to write a function similar to `knn` that computes the estimates not for the points on the training set, but for points on a new *test* set.\n",
        "\n",
        "\n",
        "* Write down the function which compute the estimated labels for a point in the test set at fixed $k$, say $k=10$\n",
        "* Compute with this function the test error\n",
        "\n",
        "**Tip**: Only distance between test point and training samples matter! Remember the caveat! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRlc8HDIBlIQ"
      },
      "outputs": [],
      "source": [
        "# Coding toghether #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "293rurCrL9dE"
      },
      "source": [
        "Great! \n",
        "\n",
        "We computed the relevant quantities which allows us to understnad how the algorithm is performing in this learning task. \n",
        "\n",
        "Still something is not clear: **How do I choose $k$?**\n",
        "\n",
        "Indeed when using $k$-nearest neighbors we have to pick a value for $k$ -- it is not clear in principle how to do it! \n",
        "\n",
        "In order to understand this better, let us look at how the training and test errors behave as a function of $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convenience functions that compute the training and test errors, given training and test samples\n",
        "def compute_train_error(X, y, k=1):\n",
        "    y_hat = knn(X, y, k)\n",
        "    return np.mean(y != y_hat)\n",
        "    \n",
        "def compute_test_error(X_train, y_train, X_test, y_test, k=1):\n",
        "    y_hat = knn_test(X_train, y_train, X_test, y_test, k)\n",
        "    return np.mean(y_test != y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "tJQR-sqY8ycz",
        "outputId": "c5ebb6ec-b75a-4841-c493-2b64065ec60a"
      },
      "outputs": [],
      "source": [
        "# Run functions for k belonging to a range of values\n",
        "ks = np.arange(1, 20)\n",
        "train_error = []\n",
        "test_error = []\n",
        "for (i, k) in enumerate(ks):\n",
        "    train_error.append(compute_train_error(X, y, k))\n",
        "    test_error.append(compute_test_error(X, y, X_test, y_test, k))\n",
        "    print(\"k = %d; train error = %g, test error = %g\" % (k, train_error[-1], test_error[-1]))\n",
        "\n",
        "# Plot results\n",
        "plt.plot(ks, train_error, label = \"train\")\n",
        "plt.plot(ks, test_error, label = \"test\")\n",
        "plt.legend()\n",
        "plt.xlabel(r\"$k$\")\n",
        "plt.ylabel(\"misclassification error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1SjvcJL8ycz"
      },
      "source": [
        "It is instructive to use another quantity in the x axis instead of $k$: the number of **degrees of freedom** $N / k$. \n",
        "\n",
        "Indeed, the larger the $k$, the smaller the number of effective parameters -- think for instance of the $k = N$ limit, where everyone is assigned the same label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluated question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Plot the test and training error as a function of $N/k$\n",
        "* Do you recognize what is going on? Have we seen this phenomena in the theoretical lectures? Discuss in groups and try to understand what is the **key** concept to grasp from this graph (which apply well beyond the specific case of $k$-nearest neighboour)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg2qk8R4Bri3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmITMyFnBrm1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3vhDq7sT964"
      },
      "source": [
        "## Bonus section ##\n",
        "\n",
        "What we have done so far is to give **estimation** of the class to which a datapoint should belong. In general we don't know where our data comes from (see later when we analyze MNIST dataset for example).\n",
        "\n",
        "Nonetheless we considered a very special case where we knew that the Generative Model was a Gaussian Mixture Model (GMM) around the sampled centroids.\n",
        "\n",
        " During theoretical classes we saw that in these cases the best thing I can do is to do Bayesian inference!\n",
        "\n",
        "Rephrasing mathematically what we have is: $$P(y = 0|x) \\propto P(x| y=0) = GMM(\\{m_i^{(0)}\\}_{i=1}^{10})  \\qquad P(y = 1|x) \\propto P(x| y=1) = GMM(\\{m_i^{(1)}\\}_{i=1}^{10}) $$\n",
        "\n",
        "\n",
        "Let's code toghether a function that assign estimates in a Bayesian way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LvgrxsgbYBvp"
      },
      "outputs": [],
      "source": [
        "def BayesFunction(class0_centroids,class1_centroids,position):\n",
        " diff0 = class0_centroids - position ; diff1 = class1_centroids - position\n",
        " exponents0 = np.sum(diff0**2,axis=1) ; exponents1 = np.sum(diff1**2,axis=1)\n",
        " p0=sum(np.exp(-2.5*exponents0))\n",
        " p1=sum(np.exp(-2.5*exponents1))\n",
        " if p0>p1:\n",
        "  return 0\n",
        " else:\n",
        "  return 1\n",
        "\n",
        "def ComputeBayesError(class0_centroids,class1_centroids,label,pos):\n",
        " frac=0\n",
        " for i in range(max(pos.shape)):\n",
        "  pred = BayesFunction(class0_centroids,class1_centroids,pos[i])\n",
        "  if pred != label[i]:\n",
        "     frac=frac+1\n",
        " return frac/max(pos.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1lj0IsraAw_"
      },
      "source": [
        "**Exercise**:\n",
        "- Compute the Bayesian error on the training and on the test set\n",
        "- Plot the Bayesian test error along with the curves of the previous exercise. How does it look like? What can you conclude?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Aqv78SzBz4O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8aHqS2qBz70"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ0p9QGwB0A0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "interpreter": {
      "hash": "71c7e7b62c8b549c17c4ebe2a10e4a9947302b4a8b96addff24585ab9f2f668b"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
