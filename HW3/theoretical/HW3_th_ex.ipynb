{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Unsupervised Learning\n",
    "\n",
    "# Part 1: Classification with K-means algorithm\n",
    "\n",
    "The K-means algorithm is a fundamental tool among the unsupervised learning model. Consider a problem with a dataset $\\mathcal{D} = \\left\\{x_i\\right\\}_{i=1}^n$ where $x_i \\in \\mathbb{R}^d$ with no labels, we are aiming at finding some hidden structure within the data, namely, we would like to find clusters in the dataset. Classifier has been studied in TP but mainly for supervised learning, here the data are not labelled. \n",
    "\n",
    "The K-means algorithm tries to classify the dataset in $K$ clusters. Each cluster is represented by a centroid, meaning the average of the points within the cluster. We note $C_k$ the set of points of a cluster $k$ and $\\mu_k$ its centroid. Then, the algorithm minimizes the intra-cluster variance, in other words, it tries to reduce the distance between the points of the cluster and the centroid. \n",
    "\n",
    "More technically, the algorithm works iteratively in two main steps: \n",
    " - Points are assigned to clusters based on their proximity to existing centroid\n",
    " - Centroids are updated by taking the average of the points assigned to each cluster.\n",
    "\n",
    "\n",
    "\n",
    "The Loss function of the problem can be written as:\n",
    "$$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 \\; ,$$\n",
    "where $\\mu(i)$ is the centroid of the cluster assignated to $x_i$. \n",
    "\n",
    "We are aiming it at checking the understanding the choice of this loss function, does it match the rules aforementionned? Then, could it help to understand if the algorithm converges?\n",
    "\n",
    "Additionnally, until Question 8, we assume that the clusters $C_k$ are disjoint, especially at the initialisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1:\n",
    "\n",
    "In a first time, let us familiarize with the loss function:\n",
    "\n",
    " - Prove the second equality:\n",
    "\n",
    "   $$J(\\mu_1, ..., \\mu_K) = \\sum_{i=1}^{n} \\, \\lVert x_i - \\mu(i)\\rVert^2 = \\sum_{k=1}^{K} \\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$$\n",
    "\n",
    " - What does the term $\\sum_{x_i \\in C_k} \\, \\lVert x_i - \\mu_k \\rVert^2$ represent?\n",
    " \n",
    " - Explain why this form of the loss function is more convenient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "\n",
    "Let us focus on the first point of the algorihtm, let us consider a single point $x_i$ and add the time dependency. Also, we denote by a $'$ the variables after the new assignement of the data points. \n",
    "\n",
    "So that, the variables are denoted by: $(\\cdot)^t \\to (\\cdot)^t\\,{}' \\to (\\cdot)^{t+1} \\to (\\cdot)^{t+1}\\,{}' \\to (\\cdot)^{t+2}$\n",
    "\n",
    "\n",
    "Thus, at each step time $t$, the new assignements of the variables leads to: (no proof required)\n",
    "\n",
    "$$ \\mu^t(i)' = {\\rm argmin}_{\\mu \\in \\left\\{\\mu^t_k\\right\\}_k} \\lVert x_i - \\mu \\rVert^2$$\n",
    "\n",
    "For instance, at time $t$ before new assignements, the vector $x_i$ belongs to a cluster $k$, while after the next assignement, it now belongs to the cluster $k'$ (it can be the same or different from the cluster $k$). \n",
    "\n",
    "\n",
    "Thus, for the whole dataset, the algorithm updates the assignement as: $\\left\\{\\mu^t(i) \\right\\} \\to \\left\\{\\mu^t(i)'\\right\\}$.\n",
    "\n",
    "\n",
    "Compare $\\lVert x_i - \\mu^t(i) \\rVert^2$ and $\\lVert x_i - \\mu^t(i)' \\rVert^2$ for a given point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3:\n",
    "\n",
    "We recall that we denote by a $'$ the variables after the new assignement of the data points, so that: $\\mu^t(i) \\to \\mu^t(i)'$ and $J_t \\to J_t'$\n",
    "\n",
    "Thanks to the previous question, compare $J_t$ and ${J_t}'$. \n",
    "\n",
    "Hint: Pick the right formula between the two given for $J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can go ahead by studying the second point of the algorithm. It wants to minimize the intra-cluster variance:\n",
    "\n",
    "$$ \\left\\{\\mu^{t+1}_k\\right\\}_k = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} J'_t\\left( \\left\\{\\mu_k\\right\\}_k \\right) = {\\rm argmin}_{\\left\\{\\mu_k\\right\\}_k \\in \\mathbb{R}^d} \\sum_{k=1}^{K} \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$\n",
    "\n",
    "\n",
    "Show that the optimization can be done cluster-wise:\n",
    "\n",
    "$$\\mu^{t+1}_k = {\\rm argmin}_{\\mu_k \\in \\mathbb{R}^d}  \\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k \\rVert^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 5\n",
    "\n",
    "Show that the new centroids of time $t+1$ are computed according to the following equality:\n",
    "\n",
    "$$ \\mu_k^{t+1} = \\frac{1}{|C^t_k{}'|}\\sum_{x_i \\in C^t_k{}'} x_i $$\n",
    "\n",
    "Does it correspond to what you expected from the algorithm? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6:\n",
    "\n",
    "If we focus on a cluster $k$ at time $t$ after the assignement, noted $C^t_k {}'$, could you compare $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^t \\rVert^2$ and $\\sum_{x_i \\in C^t_k{}'} \\, \\lVert x_i - \\mu_k^{t+1} \\rVert^2$ ?\n",
    "\n",
    "What can you say about ${J_t}'$ and $J_{t+1}$ ? Hint: Use the right formula between the two given for $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7:\n",
    "\n",
    "Putting together the Questions 3 and 5, compare $J_t$ and $J_{t+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 8:\n",
    "\n",
    "After recalling a trivial lower bound for the sequence $(J_t)_{t \\geq 0}$, what can you say about the convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Question 9:\n",
    "\n",
    "We just proved that the algorithm converges, but what about its stability:\n",
    "\n",
    "Let us supposed that the data are sampled from a mixture of $K$ Gaussian, where the choice of $K$ is free for this question. Do you imagine a situation where the algorithm does not classify the data at all? Please design and explain the situation as clear as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 10:\n",
    "\n",
    "What can you say about those configurations of centroids? What does it imply concerning the minima? Conclude your arguments by discussing the convexity of the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 11:\n",
    "\n",
    "We can also generalize quickly our algorithm\n",
    "\n",
    "In some situation you are aiming at favorizing some diretions in your data and penalysing the others, so that you can weight the euclidian distance according to:\n",
    "\n",
    "$$d^{(w)}(x_{i}, \\mu(i)) = \\frac{\\sum_{j=1}^d w_i(x_{ij} - \\mu(i)_j)^2}{\\sum_{j=1}^d w_j} $$\n",
    "\n",
    "Show that with a change of variables, the problem remains the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Restricted Boltzmann Machine\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "The Boltzmann Machine have been inspired by thermodynamic and statistical physics models, more precisely they are part of the Energy Models using the well known Boltzmann Distribution as written in physics style:\n",
    "\n",
    "$$ P\\left( E \\right)  = \\frac{1}{Z} \\exp \\left( -\\frac{E}{k_b T} \\right)$$\n",
    "\n",
    "It becomes in statistical inference framework:\n",
    "$$\n",
    "P(\\mathbf{v} | J, \\mathbf{b}) \\propto e^{\\mathbf{v}^TJ\\mathbf{v} + \\mathbf{b}^T\\mathbf{v}} = e^{-E(\\mathbf{v})}\n",
    "$$\n",
    "where:\n",
    "- $\\mathbf{v}\\in\\mathbb{R}^n:$ The binary vector with components $v_i = 0 \\; {\\rm or} \\; 1$\n",
    "\n",
    "- $J \\in \\mathbb{R}^{n \\times n}:$ The coupling matrix\n",
    "\n",
    "- $\\mathbf{b} \\in  \\mathbb{R}^n$: Field\n",
    "\n",
    "- $E(\\mathbf{v}) \\in  \\mathbb{R}$: Energy\n",
    "\n",
    "\n",
    "However, one problem arised with initial Boltzmann Machine (BM) -- like its parent models in statistical physics (as the SK model) -- all the units are interacting through complicated dependencies. For example, if we consider 3 components of $\\mathbf{v}$: $v_1$, $v_2$, and $v_3$, there are trivial interactions such as one modelised by $P(v_1, v_2)$ corresponding to the correlation between the two first components of $\\mathbf{v}$, but there are also none trivial interactions. Indeed, if some term like $P(v_1, v_2 | v_3)$ which suggests that the correlation $x_1$ and $v_2$ depends on $v_3$ and this is clearly none linear.\n",
    "\n",
    "A really ingenious way to overcome this situation is to replace all the tricky interactions between the units $\\mathbf{v}\\in\\mathbb{R}^n$ by connections through hidden units $\\mathbf{h}\\in\\mathbb{R}^m$, artifically created. Indeed, correlations between two units $v_1$ and $v_2$ (specially the dependency of their correlations on other units $v_3$, $v_4$,...) can be atrificially replaced by introducing a third unit $h_1$ and considerin only linear correlations between $v_1 \\leftrightarrow h_1$, $h_1 \\leftrightarrow v_2$ and $v_1 \\leftrightarrow v_2$. The units $v_i$ are now called the visible units. This model is the most known version of BMs. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"boltzmannmachine.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "However, this model is still fully connected and makes the computation really costful. Then, one can even simplify the model by considering zero intra layer interractions. This simplified model is call Restricted Boltzmann Machine (RBM) (Physics Nobel Price 2024 🥳).\n",
    "\n",
    "Thus, the RBM architecture consists of two layers of binary stochastic units: a $\\textbf{visible layer}$ $\\mathbf{v}$ and a $\\textbf{hidden layer}$ $\\mathbf{h}$. The layers are fully connected, but there are no connections within a layer, making the model a $\\textbf{bipartite graph}$. \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"rbm.png\" alt=\"Diagram here\" />\n",
    "</div>\n",
    "\n",
    "Restricted Boltzmann Machines (RBMs) are a class of energy-based probabilistic graphical models that are commonly used in machine learning for tasks such as dimensionality reduction, feature learning, and generative modeling.\n",
    "\n",
    "### Energy Function and Probabilities\n",
    "\n",
    "The joint configuration of the visible units $\\mathbf{v} \\in \\{0, 1\\}^d$ and the hidden units $\\mathbf{h} \\in \\{0, 1\\}^m$ is associated with an $\\textbf{energy function}$, defined as:\n",
    "\n",
    "$$ E(\\mathbf{v}, \\mathbf{h}) = -\\mathbf{v}^\\top \\mathbf{W} \\mathbf{h} - \\mathbf{b}^\\top \\mathbf{v} - \\mathbf{c}^\\top \\mathbf{h}$$\n",
    "where:\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{d \\times m}$ is the weight matrix connecting the visible and hidden units,\n",
    "- $\\mathbf{b} \\in \\mathbb{R}^d$ field of the visible units or also called the biases of the visible units,\n",
    "- $\\mathbf{c} \\in \\mathbb{R}^m$ field of the hidden units of also called the biases of the hidden units.\n",
    "\n",
    "The energy function determines the joint probability distribution over $\\mathbf{v}$ and $\\mathbf{h}$:\n",
    "$$ P(\\mathbf{v}, \\mathbf{h}) = \\frac{1}{Z} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "where $Z$ is the $\\textbf{partition function}$, ensuring normalization:\n",
    "\n",
    "$$ Z = \\sum_{\\mathbf{v}, \\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})) $$\n",
    "\n",
    "\n",
    "The marginal probability of the visible units $\\mathbf{v}$ is obtained by summing over all possible configurations of the hidden units:\n",
    "\n",
    "$$ P(\\mathbf{v}) = \\frac{1}{Z} \\sum_{\\mathbf{h}} \\exp(-E(\\mathbf{v}, \\mathbf{h})). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 12:\n",
    "\n",
    "Write a valid expression of the energy $E(\\textbf{v}, \\textbf{h})$ in the case of a BM (non-restricted) with an hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 13:\n",
    "\n",
    "One of the key properties of RBMs is the $\\textbf{conditional independence}$ between units within a layer:\n",
    "\n",
    "Compute the conditional probability and show that:\n",
    "\n",
    "$$ P(h_j = 1 | \\mathbf{v}) = \\sigma\\left(c_j + \\sum_{i} v_i W_{ij}\\right) $$\n",
    "and\n",
    "$$ P(v_i = 1 | \\mathbf{h}) = \\sigma\\left(b_i + \\sum_{j} h_j W_{ij}\\right) $$\n",
    "\n",
    "where $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ is the sigmoid activation function.\n",
    "\n",
    "This bipartite structure enables efficient Gibbs sampling for approximating the intractable joint distribution $P(\\mathbf{v}, \\mathbf{h})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in RBMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 14:\n",
    "\n",
    "Training an RBM involves maximizing the likelihood of the data distribution. To do so we are aiming at using a gradient descent/ascent on the weights (and biases).\n",
    "\n",
    "Compute the log-likelihood $\\mathcal{L}(\\mathbf{v})$, remember that the model is part of the unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 15:\n",
    "\n",
    "Compute the gradient of the log-likelihood with respect to the weights $\\mathbf{W}$ and the biases $\\mathbf{b}$, $\\mathbf{c}$ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it should be possible to implement the RBM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 16: (Open question)\n",
    "\n",
    "While it seems possible to run RBM algorithm, note that the second term in the gradient w.r.t. $\\mathbf{W}$ is computationally expensive due to the intractability of $Z$, the approximation Contrastive Divergence - k is often use. Research what is this approximation, is this approximation enough, why? Explain it with your own words and cite the papers you used for your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of RBMs\n",
    "\n",
    "RBMs are widely used in tasks such as:\n",
    "\n",
    "- $\\textbf{Dimensionality reduction}$: Similar to PCA but capable of capturing non-linear structures,\n",
    "- $\\textbf{Feature learning}$: For pre-training deep neural networks,\n",
    "- $\\textbf{Collaborative filtering}$: Used in recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
